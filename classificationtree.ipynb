{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7a473213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f84ff13",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"op_spam_v1.4\")\n",
    "neg_dir = DATA_DIR / \"negative_polarity\"\n",
    "\n",
    "sources = {\n",
    "    \"deceptive_from_MTurk\": 1, \n",
    "    \"truthful_from_Web\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6360c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for src, y in sources.items():\n",
    "    for fold_name in sorted((neg_dir / src).rglob(\"fold*\")):\n",
    "        fold_id = int(fold_name.name[-1])\n",
    "        for fp in fold_name.rglob(\"*.txt\"):\n",
    "            txt = Path(fp).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            rows.append({\"text\": txt, \"label\": y, \"fold\": fold_id, \"path\": str(fp)})\n",
    "\n",
    "df = pd.DataFrame(rows).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_mask = df[\"fold\"].isin([1,2,3,4])\n",
    "test_mask  = df[\"fold\"] == 5\n",
    "X_train, y_train, g_train = df.loc[train_mask, \"text\"], df.loc[train_mask, \"label\"], df.loc[train_mask, \"fold\"]\n",
    "X_test,  y_test           = df.loc[test_mask,  \"text\"], df.loc[test_mask,  \"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b87bea2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      feature  count\n",
      "67       room   1490\n",
      "39      hotel   1378\n",
      "17    chicago    535\n",
      "76       stay    531\n",
      "72    service    387\n",
      "..        ...    ...\n",
      "20    decided     73\n",
      "64  recommend     72\n",
      "58        pay     72\n",
      "4        away     70\n",
      "57    overall     70\n",
      "\n",
      "[94 rows x 2 columns]\n",
      "Number of features (vocabulary size): 94\n"
     ]
    }
   ],
   "source": [
    "# create new unigram\n",
    "max_count = 0.9\n",
    "min_count = 0.1\n",
    "stop_words = 'english'\n",
    "unigram_tokenizer = CountVectorizer(ngram_range=(1,1), max_df=max_count,  min_df=min_count, stop_words=stop_words)\n",
    "X_train_uni = unigram_tokenizer.fit_transform(X_train) \n",
    "\n",
    "# print some unigram information again\n",
    "features = unigram_tokenizer.get_feature_names_out()\n",
    "counts = X_train_uni.sum(axis=0).A1\n",
    "feature_counts = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'count': counts\n",
    "}).sort_values(by='count', ascending=False)\n",
    "print(feature_counts)\n",
    "print(\"Number of features (vocabulary size):\", X_train_uni.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d0c38da",
   "metadata": {},
   "source": [
    "## Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d7893",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'clf__ccp_alpha': np.float64(0.01), 'clf__criterion': 'gini', 'clf__max_depth': 7, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5}\n",
      "0.7085971528771962\n"
     ]
    }
   ],
   "source": [
    "## UNIGRAM CLASSIFICATION TREE MODEL\n",
    "#\n",
    "\n",
    "# set up pipeline for CV\n",
    "ct_pipe = Pipeline([\n",
    "    (\"tokenizer\", CountVectorizer(max_df=0.9,  min_df=5, stop_words='english', ngram_range=(1,1))),\n",
    "    (\"clf\", DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# define hyperparams\n",
    "lr_param_grid = {\n",
    "    \"clf__max_depth\": [3,5,7],\n",
    "    'clf__min_samples_leaf': [1,2],\n",
    "    'clf__min_samples_split': [2,5],\n",
    "    'clf__max_features': ['sqrt', 'log2'],  # given the massive vocabulary, this seems okay\n",
    "    \"clf__criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "    \"clf__ccp_alpha\": np.arange(0.01, 0.11, 0.01)   \n",
    "}\n",
    "\n",
    "# perform cv\n",
    "grid = GridSearchCV(ct_pipe, lr_param_grid, scoring=\"f1\", cv=4, error_score='raise')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# evaluate cv\n",
    "results = grid.cv_results_\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best f1:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8c8dcd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set for best unigram model:\n",
      "f1: 0.6309\n",
      "accuracy: 0.6562\n",
      "precision: 0.6812\n",
      "recall: 0.5875\n",
      "\n",
      "Confusion matrix:\n",
      " [[58 22]\n",
      " [33 47]]\n"
     ]
    }
   ],
   "source": [
    "# General performance of best model\n",
    "best_pipe = grid.best_estimator_\n",
    "best_tokenizer = best_pipe.named_steps['tokenizer']\n",
    "best_clf = best_pipe.named_steps['clf']\n",
    "\n",
    "y_pred = best_clf.predict(best_tokenizer.transform(X_test))\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", pos_label=1)\n",
    "print(f\"Evaluation on test set for best unigram model:\" )\n",
    "print(f\"f1: {f1:.4f}\")\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"precision: {p:.4f}\")\n",
    "print(f\"recall: {r:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "67ec2bba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features unigram model:\n",
      "chicago: 0.7129\n",
      "windows: 0.0948\n",
      "michigan: 0.0818\n",
      "stated: 0.0669\n",
      "peeling: 0.0435\n"
     ]
    }
   ],
   "source": [
    "# top 5 features\n",
    "feature_names = best_tokenizer.get_feature_names_out()\n",
    "feature_importances = best_clf.feature_importances_\n",
    "indices = np.argsort(feature_importances)[-5:][::-1]\n",
    "\n",
    "print(\"Top 5 features unigram model:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baaf2c27",
   "metadata": {},
   "source": [
    "Performance is pretty terrible, however the Chicago feature pops up again as an important one. Maybe an indication that the model is learning, but terrible for the task. Pruning the vocabulary more did not result in better accuracy either (in fact the opposite)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74cc64e5",
   "metadata": {},
   "source": [
    "## Unigram + bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "37ada491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf__ccp_alpha': np.float64(0.01), 'clf__criterion': 'gini', 'clf__max_depth': 7, 'clf__max_features': 'sqrt', 'clf__min_samples_leaf': 1, 'clf__min_samples_split': 5}\n",
      "Best f1: 0.6232993617525697\n"
     ]
    }
   ],
   "source": [
    "## UNIGRAM + BIGRAM\n",
    "#\n",
    "\n",
    "# set up pipeline for CV\n",
    "ct_pipe = Pipeline([\n",
    "    (\"tokenizer\", CountVectorizer(max_df=0.9,  min_df=5, stop_words='english', ngram_range=(1,2))), # <-- unigram+bigram\n",
    "    (\"clf\", DecisionTreeClassifier())\n",
    "])\n",
    "\n",
    "# define hyperparams\n",
    "lr_param_grid = {\n",
    "    \"clf__max_depth\": [3,5,7],\n",
    "    'clf__min_samples_leaf': [1,2],\n",
    "    'clf__min_samples_split': [2,5],\n",
    "    'clf__max_features': ['sqrt', 'log2'],  # given the massive vocabulary, this seems okay\n",
    "    \"clf__criterion\": ['gini', 'entropy', 'log_loss'],\n",
    "    \"clf__ccp_alpha\": np.arange(0.01, 0.11, 0.01)   \n",
    "}\n",
    "\n",
    "# perform cv\n",
    "grid = GridSearchCV(ct_pipe, lr_param_grid, scoring=\"f1\", cv=4, error_score='raise')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# evaluate cv\n",
    "results = grid.cv_results_\n",
    "\n",
    "print(\"Best params:\", grid.best_params_)\n",
    "print(\"Best f1:\", grid.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ff77e8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set for best unigram model:\n",
      "f1: 0.6731\n",
      "accuracy: 0.5750\n",
      "precision: 0.5469\n",
      "recall: 0.8750\n",
      "\n",
      "Confusion matrix:\n",
      " [[22 58]\n",
      " [10 70]]\n"
     ]
    }
   ],
   "source": [
    "# General performance of best model\n",
    "best_pipe = grid.best_estimator_\n",
    "best_tokenizer = best_pipe.named_steps['tokenizer']\n",
    "best_clf = best_pipe.named_steps['clf']\n",
    "\n",
    "y_pred = best_clf.predict(best_tokenizer.transform(X_test))\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", pos_label=1)\n",
    "print(f\"Evaluation on test set for best unigram model:\" )\n",
    "print(f\"f1: {f1:.4f}\")\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"precision: {p:.4f}\")\n",
    "print(f\"recall: {r:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "290f0275",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features unigram model:\n",
      "location: 0.8265\n",
      "hilton chicago: 0.1735\n",
      "young: 0.0000\n",
      "yes: 0.0000\n",
      "yelling: 0.0000\n"
     ]
    }
   ],
   "source": [
    "# top 5 features\n",
    "feature_names = best_tokenizer.get_feature_names_out()\n",
    "feature_importances = best_clf.feature_importances_\n",
    "indices = np.argsort(feature_importances)[-5:][::-1]\n",
    "\n",
    "print(\"Top 5 features unigram model:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46920111",
   "metadata": {},
   "source": [
    "This does look wrong."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
