{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "a680ba23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "c735be9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"op_spam_v1.4\")\n",
    "neg_dir = DATA_DIR / \"negative_polarity\"\n",
    "\n",
    "sources = {\n",
    "    \"deceptive_from_MTurk\": 1, \n",
    "    \"truthful_from_Web\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cebb19f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for src, y in sources.items():\n",
    "    for fold_name in sorted((neg_dir / src).rglob(\"fold*\")):\n",
    "        fold_id = int(fold_name.name[-1])\n",
    "        for fp in fold_name.rglob(\"*.txt\"):\n",
    "            txt = Path(fp).read_text(encoding=\"utf-8\", errors=\"ignore\")\n",
    "            rows.append({\"text\": txt, \"label\": y, \"fold\": fold_id, \"path\": str(fp)})\n",
    "\n",
    "df = pd.DataFrame(rows).sample(frac=1.0, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_mask = df[\"fold\"].isin([1,2,3,4])\n",
    "test_mask  = df[\"fold\"] == 5\n",
    "X_train, y_train, g_train = df.loc[train_mask, \"text\"], df.loc[train_mask, \"label\"], df.loc[train_mask, \"fold\"]\n",
    "X_test,  y_test           = df.loc[test_mask,  \"text\"], df.loc[test_mask,  \"label\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "57abe757",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  count\n",
      "6111       the   7542\n",
      "6209        to   3525\n",
      "426        and   3340\n",
      "6669       was   2812\n",
      "3173        in   1945\n",
      "...        ...    ...\n",
      "2177   enemies      1\n",
      "2176  enduring      1\n",
      "6878       yrs      1\n",
      "6877     youth      1\n",
      "6875     yours      1\n",
      "\n",
      "[6885 rows x 2 columns]\n",
      "Number of features (vocabulary size): 6885\n",
      "Number of features with 1 occurrence: 3208\n",
      "Number of features with counts <5: 5037\n"
     ]
    }
   ],
   "source": [
    "# create unigrams\n",
    "unigram_tokenizer = CountVectorizer(ngram_range=(1,1))\n",
    "X_train_uni = unigram_tokenizer.fit_transform(X_train) \n",
    "\n",
    "# print some unigram information\n",
    "features = unigram_tokenizer.get_feature_names_out()\n",
    "counts = X_train_uni.sum(axis=0).A1\n",
    "feature_counts = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'count': counts\n",
    "}).sort_values(by='count', ascending=False)\n",
    "print(feature_counts)\n",
    "print(\"Number of features (vocabulary size):\", X_train_uni.shape[1])\n",
    "print(\"Number of features with 1 occurrence:\", np.sum(counts == 1))\n",
    "print(\"Number of features with counts <5:\", np.sum(counts < 5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde3ccf",
   "metadata": {},
   "source": [
    "The most common features appear to be mostly uninformative stop words (the, to, in, etc). Furthermore, since more than 70% of the features have low counts (<5), and almost 50% have a count of exactly 1, we decide to prune the vocabulary using the following methods:\n",
    "- Removing common stopwords\n",
    "- Removing all features with frequency >90%\n",
    "- Removing all features with counts <5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d6ced4c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       feature  count\n",
      "1150      room   1490\n",
      "676      hotel   1378\n",
      "248    chicago    535\n",
      "1294      stay    531\n",
      "1190   service    387\n",
      "...        ...    ...\n",
      "894      nasty      5\n",
      "1477    washed      5\n",
      "86      appear      5\n",
      "841   meantime      5\n",
      "884       mood      5\n",
      "\n",
      "[1535 rows x 2 columns]\n",
      "Number of features (vocabulary size): 1535\n"
     ]
    }
   ],
   "source": [
    "# create new unigram\n",
    "max_count = 0.9\n",
    "min_count = 5\n",
    "stop_words = 'english'\n",
    "unigram_tokenizer = CountVectorizer(ngram_range=(1,1), max_df=max_count,  min_df=min_count, stop_words=stop_words)\n",
    "X_train_uni = unigram_tokenizer.fit_transform(X_train) \n",
    "\n",
    "# print some unigram information again\n",
    "features = unigram_tokenizer.get_feature_names_out()\n",
    "counts = X_train_uni.sum(axis=0).A1\n",
    "feature_counts = pd.DataFrame({\n",
    "    'feature': features,\n",
    "    'count': counts\n",
    "}).sort_values(by='count', ascending=False)\n",
    "print(feature_counts)\n",
    "print(\"Number of features (vocabulary size):\", X_train_uni.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d256ea",
   "metadata": {},
   "source": [
    "## Unigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "6907e653",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 | C\n",
      "\n",
      "Results for unigram models:\n",
      "0.6875: 0.0001\n",
      "0.7195: 0.001\n",
      "0.8262: 0.01\n",
      "0.8462: 0.1\n",
      "0.8455: 1.0\n",
      "0.8417: 10.0\n",
      "0.8402: 100.0\n",
      "Best unigram model params: {'clf__C': np.float64(0.1)}\n"
     ]
    }
   ],
   "source": [
    "## UNIGRAM MODEL\n",
    "## \n",
    "\n",
    "# set up pipeline for CV\n",
    "lr_pipe = Pipeline([\n",
    "    (\"tokenizer\", CountVectorizer(max_df=0.9,  min_df=5, stop_words='english', ngram_range=(1,1))),\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# define hyperparams\n",
    "C_list = np.logspace(-4, 2, 7)\n",
    "lr_param_grid = {\n",
    "    \"clf__C\": C_list \n",
    "}\n",
    "\n",
    "# perform cv\n",
    "grid = GridSearchCV(lr_pipe, lr_param_grid, scoring=\"f1\", cv=4)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "# evaluate cv\n",
    "results = grid.cv_results_\n",
    "\n",
    "# print unigram results\n",
    "print(\"f1 | C\\n\")\n",
    "print(\"Results for unigram models:\")\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"{mean_score:.4f}: {params['clf__C']}\")\n",
    "\n",
    "print(f\"Best unigram model params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "da334de6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set for best unigram model:\n",
      "f1: 0.8312\n",
      "accuracy: 0.8375\n",
      "precision: 0.8649\n",
      "recall: 0.8000\n",
      "\n",
      "Confusion matrix:\n",
      " [[70 10]\n",
      " [16 64]]\n"
     ]
    }
   ],
   "source": [
    "# General performance of best model\n",
    "best_pipe = grid.best_estimator_\n",
    "best_tokenizer = best_pipe.named_steps['tokenizer']\n",
    "best_clf = best_pipe.named_steps['clf']\n",
    "\n",
    "y_pred = best_clf.predict(best_tokenizer.transform(X_test))\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", pos_label=1)\n",
    "print(f\"Evaluation on test set for best unigram model:\" )\n",
    "print(f\"f1: {f1:.4f}\")\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"precision: {p:.4f}\")\n",
    "print(f\"recall: {r:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "66d42329",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features unigram model:\n",
      "chicago: 0.7273\n",
      "location: -0.4487\n",
      "star: -0.4486\n",
      "finally: 0.4292\n",
      "luxury: 0.4211\n"
     ]
    }
   ],
   "source": [
    "# top 5 features\n",
    "feature_names = best_tokenizer.get_feature_names_out()\n",
    "feature_importances = best_clf.coef_[0]\n",
    "indices = np.argsort(np.abs(feature_importances))[-5:][::-1] # sort top 5 by absolute value of feature weight\n",
    "\n",
    "print(\"Top 5 features unigram model:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importances[i]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126760c4",
   "metadata": {},
   "source": [
    "## Unigram + bigram model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "9c3396ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 | C\n",
      "\n",
      "Results for unigram+bigram models:\n",
      "0.6821: 0.0001\n",
      "0.7278: 0.001\n",
      "0.8351: 0.01\n",
      "0.8540: 0.1\n",
      "0.8517: 1.0\n",
      "0.8548: 10.0\n",
      "0.8438: 100.0\n",
      "Best unigram+bigram model params: {'clf__C': np.float64(10.0)}\n"
     ]
    }
   ],
   "source": [
    "## UNIGRAM+BIGRAM MODEL\n",
    "## \n",
    "\n",
    "# set up pipeline for CV\n",
    "lr_pipe = Pipeline([\n",
    "    (\"tokenizer\", CountVectorizer(max_df=0.9,  min_df=5, stop_words='english', ngram_range=(1,2))), # <-- unigram+bigram\n",
    "    (\"clf\", LogisticRegression())\n",
    "])\n",
    "\n",
    "# define hyperparams\n",
    "C_list = np.logspace(-4, 2, 7)\n",
    "lr_param_grid = {\n",
    "    \"clf__C\": C_list\n",
    "}\n",
    "\n",
    "# perform cv\n",
    "grid = GridSearchCV(lr_pipe, lr_param_grid, scoring=\"f1\", cv=4)\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "# evaluate cv\n",
    "results = grid.cv_results_\n",
    "\n",
    "# print unigram results\n",
    "print(\"f1 | C\\n\")\n",
    "print(\"Results for unigram+bigram models:\")\n",
    "for mean_score, params in zip(results['mean_test_score'], results['params']):\n",
    "    print(f\"{mean_score:.4f}: {params['clf__C']}\")\n",
    "\n",
    "print(f\"Best unigram+bigram model params: {grid.best_params_}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "88b5c316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation on test set for best unigram model:\n",
      "f1: 0.8182\n",
      "accuracy: 0.8250\n",
      "precision: 0.8514\n",
      "recall: 0.7875\n",
      "\n",
      "Confusion matrix:\n",
      " [[69 11]\n",
      " [17 63]]\n"
     ]
    }
   ],
   "source": [
    "# General performance of best model\n",
    "best_pipe = grid.best_estimator_\n",
    "best_tokenizer = best_pipe.named_steps['tokenizer']\n",
    "best_clf = best_pipe.named_steps['clf']\n",
    "\n",
    "y_pred = best_clf.predict(best_tokenizer.transform(X_test))\n",
    "p, r, f1, _ = precision_recall_fscore_support(y_test, y_pred, average=\"binary\", pos_label=1)\n",
    "print(f\"Evaluation on test set for best unigram model:\" )\n",
    "print(f\"f1: {f1:.4f}\")\n",
    "print(f\"accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"precision: {p:.4f}\")\n",
    "print(f\"recall: {r:.4f}\")\n",
    "\n",
    "print(\"\\nConfusion matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5f4c0b58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 5 features unigram model:\n",
      "star: -1.4659\n",
      "chicago: 1.4299\n",
      "recently: 1.2207\n",
      "concierge: -1.2200\n",
      "location: -1.2193\n"
     ]
    }
   ],
   "source": [
    "feature_names = best_tokenizer.get_feature_names_out()\n",
    "feature_importances = best_clf.coef_[0]\n",
    "indices = np.argsort(np.abs(feature_importances))[-5:][::-1] # sort top 5 by absolute value of feature weight\n",
    "\n",
    "print(\"Top 5 features unigram model:\")\n",
    "for i in indices:\n",
    "    print(f\"{feature_names[i]}: {feature_importances[i]:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
